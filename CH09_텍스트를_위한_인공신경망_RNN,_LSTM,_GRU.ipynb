{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH09 텍스트를 위한 인공신경망 : RNN, LSTM, GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPO+fwvRfSLZRTtRcpM3coQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YebinLeee/Machine-Learning-Study/blob/main/CH09_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A5%BC_%EC%9C%84%ED%95%9C_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_RNN%2C_LSTM%2C_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex1jgqry5wrt"
      },
      "source": [
        "<h1> Machine Learning Seminar </h1>\n",
        "\n",
        "- CH 09 텍스트를 위한 인공 신경망\n",
        "- 2021.10.12\n",
        "\n",
        "<br><hr><br>\n",
        "\n",
        "<h2> 순차 데이터 (Sequential Data) </h2>\n",
        "\n",
        "- 순차 데이터: 순서가 의미가 있으며, 순서가 달라질 경우 의미가 손상되는 데이터\n",
        "- 시간적 의미가 있는 경우 Temporal Sequence / 일정한 시간차라면 Time Series (시계열)\n",
        "- ex) DNA 염기서열, 세계 기온변화, 샘플링된 소리 신호, 주식 데이터\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> Resampling </h2>\n",
        "\n",
        ": Temporal Sequence를 Time Series로 변환하기 위한 리샘플링(Resampling) 과정\n",
        "- 취득된 데이터(Temporal Sequence)를 이용해 신호 보간 (Interpolation)하고, 균일 시간 간격으로 이를 샘플링 하는 과정\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> RNN (Recurrent Neural Network) </h2>\n",
        "\n",
        "- 타임 스텝으로 펼쳐진 순환 신경망\n",
        "- 셀: 순환층, 활성화 함수(tanh - hyperbolic tangent function)에 의한 값이 순환적으로 입력됨\n",
        "- 각 타임 스텝마다 동일한 가중치를 사용(가중치 공유)\n",
        "- 출력 결과 값(은닉 상태:Hidden State)\n",
        "\n",
        "<br>\n",
        "\n",
        "- 셀의 가중치 : 순환층에 입력되는 특성의 개수 * 순환층의 뉴런 개수\n",
        "- 시퀀스: 하나의 샘플\n",
        "\n",
        "- 토큰 : 하나의 단어를 정수로 매핑 (1개의 토큰 -> 1 타임스텝)\n",
        "\n",
        "<br><br><hr><br>\n",
        "\n",
        "\n",
        "<h2> 순환 신경망으로 IMDB 리뷰 분류하기 <h2>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcYcqsp4_vjd",
        "outputId": "b0a7ad96-780f-44ec-b75c-4ccf4aa30f3f"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "(train_input, train_target), (test_input, test_target) = imdb.load_data(\n",
        "    num_words=500) # num_words = 어휘 사전을 500개 사용하도록\n",
        "\n",
        "print(train_input.shape, test_input.shape) # train_input [리뷰 1, 리뷰2, 리뷰3...] (25000,) (25000,)\n",
        "\n",
        "\n",
        "print(len(train_input[0])) # 218: 첫번째 리뷰의 토큰: 218개\n",
        "\n",
        "\n",
        "print(len(train_input[1])) # 189: 두번째 리뷰의 토큰: 189개\n",
        "\n",
        "\n",
        "print(train_input[0]) # 첫번째 리뷰의 토큰(정수)들 모두 출력\n",
        "\n",
        "# 타깃 데이터 출력\n",
        "print(train_target[:20]) # 0:부정, 1:긍정"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,) (25000,)\n",
            "218\n",
            "189\n",
            "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Sm2PUTNYA64r",
        "outputId": "1bb7f8e4-a314-4f94-c1bc-c3edfd26dc48"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 훈련, 테스트 데이터 분리 (훈련: 20,000개, 테스트: 5,000개)\n",
        "train_input, val_input, train_target, val_target = train_test_split(\n",
        "    train_input, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "import numpy as np\n",
        "lengths = np.array([len(x) for x in train_input]) # 모든 리뷰의 길이\n",
        "print(np.mean(lengths), np.median(lengths)) # 리뷰 길이의 평균값(239개의 토큰값), 중간값(178) => 매우 긴 리뷰들이 소수 있음을 확인할 수 있음\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(lengths)\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239.00925 178.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mtevPq5Bprb"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h3> 패딩 작업 </h3>\n",
        "- 비어있는 토큰 자리를 모두 0으로 채워 넣는 작업, 또는 지정한 길이보다 긴 시퀀스를 자르는 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDGUQWrTB_RM",
        "outputId": "67763c64-c94f-4071-e9f7-9632ccfdfcba"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 입력 데이터의 길이를 100개의 토큰 길이로 잡는다. (maxlen = 100), 100보다 작으면 sequence padding과정이 이루어짐 -> 타임 스텝의 개수: 100\n",
        "train_seq = pad_sequences(train_input, maxlen=100) # maxlen 보다 긴 시퀀스의 앞부분을 자름 (뒷 부분의 정보가 더 유용하리라 기대하기 때문)\n",
        "\n",
        "print(train_seq.shape) # (20000, 100) -> 20,000개의 리뷰 데이터, 100개의 타임 스탭(토큰 개수)\n",
        "\n",
        "\n",
        "print(train_seq[0])\n",
        "\n",
        "print(train_input[0][-10:]) # 앞부분이 0으로 채워져있음을 확인 가능\n",
        "\n",
        "\n",
        "print(train_seq[5])\n",
        "\n",
        "\n",
        "val_seq = pad_sequences(val_input, maxlen=100)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100)\n",
            "[ 10   4  20   9   2 364 352   5  45   6   2   2  33 269   8   2 142   2\n",
            "   5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93\n",
            "  76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6\n",
            "   2   7   2   2 188   2 103  14  31  10  10 451   7   2   5   2  80  91\n",
            "   2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79\n",
            "   6   2  46   7  14  20  10  10 470 158]\n",
            "[6, 2, 46, 7, 14, 20, 10, 10, 470, 158]\n",
            "[  0   0   0   0   1   2 195  19  49   2   2 190   4   2 352   2 183  10\n",
            "  10  13  82  79   4   2  36  71 269   8   2  25  19  49   7   4   2   2\n",
            "   2   2   2  10  10  48  25  40   2  11   2   2  40   2   2   5   4   2\n",
            "   2  95  14 238  56 129   2  10  10  21   2  94 364 352   2   2  11 190\n",
            "  24 484   2   7  94 205 405  10  10  87   2  34  49   2   7   2   2   2\n",
            "   2   2 290   2  46  48  64  18   4   2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UhlNlofD5B7"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h3> 순환 신경망 모델 만들기 </h3\n",
        "\n",
        "- SimpleRNN 클래스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiMTsAjmC3vj",
        "outputId": "cffaedc0-5f27-45b7-fb1a-eea3e3d54333"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500))) # 100개의 타임 스텝, 어휘 사전의 500개의 단어(토큰)을 사용하도록 설정\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# 원-핫 인코딩 (500개의 원소 중 1개를 제외하고 나머지는 0값으로 설정)\n",
        "train_oh = keras.utils.to_categorical(train_seq) \n",
        "print(train_oh.shape) # (20000, 100, 500) -> 20,000개의 데이터 , 100개의 타임 스텝, 500개의 어휘 사전\n",
        "\n",
        "\n",
        "print(train_oh[0][0][:12])\n",
        "\n",
        "\n",
        "print(np.sum(train_oh[0][0]))\n",
        "\n",
        "\n",
        "val_oh = keras.utils.to_categorical(val_seq)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 4072 개 (500개 어휘사전 * 뉴런 8개 + 64개의 가중치(은닉상태크기*뉴런 개수) + 절편8)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 500)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "1.0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 8)                 4072      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 4,081\n",
            "Trainable params: 4,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxtrziPZFBdC"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h3> 순환 신경망 훈련 </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ3abU9tE_vb",
        "outputId": "29c345c5-2a31-4954-e66e-6eb75ab56c44"
      },
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', \n",
        "                                                save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "\n",
        "# RMSprop 객체 별도로 생성하여 학습률을 0.0001로 지정\n",
        "# 에포크 횟수 100, 배치 크기 64\n",
        "history = model.fit(train_oh, train_target, epochs=100, batch_size=64,\n",
        "                    validation_data=(val_oh, val_target),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 15s 45ms/step - loss: 0.6956 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5166\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.6883 - accuracy: 0.5429 - val_loss: 0.6824 - val_accuracy: 0.5722\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.6752 - accuracy: 0.5907 - val_loss: 0.6702 - val_accuracy: 0.6026\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.6580 - accuracy: 0.6347 - val_loss: 0.6504 - val_accuracy: 0.6526\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.6380 - accuracy: 0.6724 - val_loss: 0.6326 - val_accuracy: 0.6774\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.6189 - accuracy: 0.6962 - val_loss: 0.6150 - val_accuracy: 0.7016\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.6003 - accuracy: 0.7164 - val_loss: 0.5977 - val_accuracy: 0.7088\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.5824 - accuracy: 0.7320 - val_loss: 0.5814 - val_accuracy: 0.7280\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.5655 - accuracy: 0.7430 - val_loss: 0.5729 - val_accuracy: 0.7296\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.5510 - accuracy: 0.7531 - val_loss: 0.5552 - val_accuracy: 0.7396\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.5364 - accuracy: 0.7631 - val_loss: 0.5439 - val_accuracy: 0.7446\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.5245 - accuracy: 0.7699 - val_loss: 0.5310 - val_accuracy: 0.7578\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.5131 - accuracy: 0.7760 - val_loss: 0.5231 - val_accuracy: 0.7632\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.5027 - accuracy: 0.7804 - val_loss: 0.5151 - val_accuracy: 0.7642\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.4939 - accuracy: 0.7852 - val_loss: 0.5056 - val_accuracy: 0.7730\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.4855 - accuracy: 0.7874 - val_loss: 0.4980 - val_accuracy: 0.7762\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.4780 - accuracy: 0.7895 - val_loss: 0.4932 - val_accuracy: 0.7808\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4708 - accuracy: 0.7940 - val_loss: 0.4869 - val_accuracy: 0.7796\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4645 - accuracy: 0.7977 - val_loss: 0.4830 - val_accuracy: 0.7818\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4591 - accuracy: 0.8007 - val_loss: 0.4883 - val_accuracy: 0.7728\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4539 - accuracy: 0.8033 - val_loss: 0.4800 - val_accuracy: 0.7832\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4491 - accuracy: 0.8034 - val_loss: 0.4716 - val_accuracy: 0.7816\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4449 - accuracy: 0.8070 - val_loss: 0.4752 - val_accuracy: 0.7836\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4408 - accuracy: 0.8087 - val_loss: 0.4701 - val_accuracy: 0.7838\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4372 - accuracy: 0.8101 - val_loss: 0.4659 - val_accuracy: 0.7848\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4337 - accuracy: 0.8098 - val_loss: 0.4664 - val_accuracy: 0.7842\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4310 - accuracy: 0.8119 - val_loss: 0.4628 - val_accuracy: 0.7858\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.4284 - accuracy: 0.8135 - val_loss: 0.4592 - val_accuracy: 0.7870\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.4250 - accuracy: 0.8149 - val_loss: 0.4623 - val_accuracy: 0.7858\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4228 - accuracy: 0.8153 - val_loss: 0.4588 - val_accuracy: 0.7858\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4204 - accuracy: 0.8151 - val_loss: 0.4551 - val_accuracy: 0.7880\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4188 - accuracy: 0.8170 - val_loss: 0.4578 - val_accuracy: 0.7904\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.4163 - accuracy: 0.8172 - val_loss: 0.4585 - val_accuracy: 0.7884\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4143 - accuracy: 0.8180 - val_loss: 0.4561 - val_accuracy: 0.7894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e482g06HFg4S"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "원-핫 인코딩으로는 앞뒤 단어들과의 관계를 살펴보기 어려움\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<h2> 단어 임베딩 (Word Embedding) </h2>\n",
        "\n",
        "- 단어들끼리의 연관성 추가\n",
        "- 각 단어를 고정된 크기의 실수 벡터로 바꾸어줌\n",
        "- Embedding 클래스 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW5tZ8hgGD_z",
        "outputId": "8c507d9f-5c55-40d8-e268-2edb10116e4b"
      },
      "source": [
        "model2 = keras.Sequential()\n",
        "\n",
        "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model2.add(keras.layers.SimpleRNN(8))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
        "               metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', \n",
        "                                                save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
        "                     validation_data=(val_seq, val_target),\n",
        "                     callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           8000      \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 8)                 200       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 8,209\n",
            "Trainable params: 8,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VsOPrSJGfsT"
      },
      "source": [
        "<br>\n",
        "\n",
        "RNN의 한계점\n",
        "- 기본 순환층은 긴 시퀀스를 학습하기 어려움 -> 시퀀스가 길수록 은닉 상태에 담긴 정보가 점차 희석되며, 멀리 떨어져 있는 단어 정보를 인식하는데 어려움이 있음\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h2> LSTM (Long Short-Term Memory </h2>\n",
        "\n",
        "- RNN의 한계점을 보완하여, 단기 기억을 오래 기억하기 위해 고안된 모델\n",
        "- 순환되는 상태가 2 개 (은닉 상태 + 셀 상태)\n",
        "\n",
        "<br>\n",
        "\n",
        "<h3> Cell State </h3>\n",
        "\n",
        "- 다음 층으로 전달되지 않고 LSTM 셀에서 순환만 되는 값\n",
        "- 셀 상태의 계산 과정 -> 입력과 은닉 상태를 또다른 가중치에 곱한 후 시그모이드 함수를 통화시키고, 이전 타임스텝의 셀상태와 곱하여 새로운 셀 상태 생성 -> tanh 함수를 통과하여 새로운 은닉 상태 생성\n",
        "<br>\n",
        "\n",
        "- Forget gate layer : 셀 상태에 있는 정보를 제거하는 역할의 삭제 게이트\n",
        "- Input gate layer : 새로운 정보를 셀 상태에 추가하는 역할의 입력 게이트\n",
        "- Output gate layer : 셀 상태가 다음 은닉 상태로 출력하는 역할의 출력 게이트\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h3> LSTM 신경망 훈련 </h3>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwQpfpSwIshI",
        "outputId": "911298e7-23c7-4497-9b2e-0a4a25243b4f"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_input, train_target), (test_input, test_target) = imdb.load_data(\n",
        "    num_words=500)\n",
        "\n",
        "train_input, val_input, train_target, val_target = train_test_split(\n",
        "    train_input, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "train_seq = pad_sequences(train_input, maxlen=100)\n",
        "val_seq = pad_sequences(val_input, maxlen=100)\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "\n",
        "model.add(keras.layers.LSTM(8)) # Embedding 이후, SimpleRNN 이 아닌 LSTM 클래스 사용\n",
        "# 작은 셀 4개 -> 200에서 4배 늘어 800개의 파라미터 값을 갖게 됨\n",
        "\n",
        "\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 16)           8000      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8)                 800       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 8,809\n",
            "Trainable params: 8,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yx0MysiK0ha"
      },
      "source": [
        "<h3> LSTM 모델 훈련 </h3>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4t5zOK_KZQL",
        "outputId": "6ff3ea5d-8934-40a3-872e-d1217b0b4ae6"
      },
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', \n",
        "                                                save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
        "                    validation_data=(val_seq, val_target),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 14s 37ms/step - loss: 0.6920 - accuracy: 0.5553 - val_loss: 0.6908 - val_accuracy: 0.5986\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.6871 - accuracy: 0.6452 - val_loss: 0.6817 - val_accuracy: 0.6616\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.6513 - accuracy: 0.6945 - val_loss: 0.6193 - val_accuracy: 0.6884\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.6007 - accuracy: 0.7057 - val_loss: 0.5919 - val_accuracy: 0.7106\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.5740 - accuracy: 0.7311 - val_loss: 0.5671 - val_accuracy: 0.7388\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.5527 - accuracy: 0.7494 - val_loss: 0.5494 - val_accuracy: 0.7434\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.5331 - accuracy: 0.7620 - val_loss: 0.5321 - val_accuracy: 0.7540\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.5146 - accuracy: 0.7729 - val_loss: 0.5145 - val_accuracy: 0.7676\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4973 - accuracy: 0.7825 - val_loss: 0.5005 - val_accuracy: 0.7688\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4812 - accuracy: 0.7892 - val_loss: 0.4849 - val_accuracy: 0.7808\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4683 - accuracy: 0.7946 - val_loss: 0.4755 - val_accuracy: 0.7868\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4586 - accuracy: 0.7986 - val_loss: 0.4665 - val_accuracy: 0.7868\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4509 - accuracy: 0.7998 - val_loss: 0.4627 - val_accuracy: 0.7890\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4455 - accuracy: 0.8027 - val_loss: 0.4611 - val_accuracy: 0.7890\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4405 - accuracy: 0.8055 - val_loss: 0.4550 - val_accuracy: 0.7890\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4367 - accuracy: 0.8055 - val_loss: 0.4569 - val_accuracy: 0.7906\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4336 - accuracy: 0.8073 - val_loss: 0.4499 - val_accuracy: 0.7934\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4304 - accuracy: 0.8095 - val_loss: 0.4527 - val_accuracy: 0.7900\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4280 - accuracy: 0.8080 - val_loss: 0.4468 - val_accuracy: 0.7958\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4254 - accuracy: 0.8105 - val_loss: 0.4461 - val_accuracy: 0.7952\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4237 - accuracy: 0.8110 - val_loss: 0.4449 - val_accuracy: 0.7942\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4212 - accuracy: 0.8112 - val_loss: 0.4446 - val_accuracy: 0.7944\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4195 - accuracy: 0.8117 - val_loss: 0.4423 - val_accuracy: 0.7976\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4177 - accuracy: 0.8143 - val_loss: 0.4436 - val_accuracy: 0.7986\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4163 - accuracy: 0.8149 - val_loss: 0.4418 - val_accuracy: 0.7990\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4150 - accuracy: 0.8152 - val_loss: 0.4387 - val_accuracy: 0.7972\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4140 - accuracy: 0.8151 - val_loss: 0.4402 - val_accuracy: 0.7982\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4128 - accuracy: 0.8137 - val_loss: 0.4380 - val_accuracy: 0.7998\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4117 - accuracy: 0.8152 - val_loss: 0.4372 - val_accuracy: 0.7960\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4104 - accuracy: 0.8170 - val_loss: 0.4370 - val_accuracy: 0.7960\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4095 - accuracy: 0.8170 - val_loss: 0.4355 - val_accuracy: 0.7974\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4088 - accuracy: 0.8160 - val_loss: 0.4354 - val_accuracy: 0.8012\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4075 - accuracy: 0.8169 - val_loss: 0.4350 - val_accuracy: 0.7988\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4070 - accuracy: 0.8168 - val_loss: 0.4367 - val_accuracy: 0.8032\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4061 - accuracy: 0.8177 - val_loss: 0.4382 - val_accuracy: 0.7932\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.4051 - accuracy: 0.8178 - val_loss: 0.4355 - val_accuracy: 0.7984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MXl2C3ZKaYp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_sR6S8K60T"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "<h3> 순환층에 드롭아웃 적용하기 </h3>\n",
        "\n",
        "- 순환층의 자체적인 드롭아웃 기능 사용 (과대적합 예방 - train data에 의존성 막기 위함)\n",
        "\n",
        "1. dropout 매개변수 - 셀의 입력에 드롭아웃 적용\n",
        "2. recurrent_dropout 매개변수 - 순환되는 은닉 상태에 드롭아웃 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "l0MqXK9NKbZq",
        "outputId": "46164042-5074-42cc-f0c5-5ba3a039a3f0"
      },
      "source": [
        "model2 = keras.Sequential()\n",
        "\n",
        "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model2.add(keras.layers.LSTM(8, dropout=0.3)) # dropout  0.3 (30%입력 드롭아웃 적용)\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid’))\n",
        "\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
        "               metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-dropout-model.h5', \n",
        "                                                save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
        "                     validation_data=(val_seq, val_target),\n",
        "                     callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-9d6c4ee87fd6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    model2.add(keras.layers.Dense(1, activation='sigmoid’))\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Dxj2DeKcmK"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euoPXpi8METo"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h3> 2개의 층 연결하기 </h3>\n",
        "\n",
        "- 순환층 연결 할 때 은닉 상태를 마지막 타임 스텝에 대해 다음층으로 전달하기 위해, return_sequences 매개변수 True로 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV6gOGWUKdYK"
      },
      "source": [
        "model3 = keras.Sequential()\n",
        "\n",
        "model3.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True)) # 앞쪽의 순환층이 모든 타임스텝에 대한 은닉상태 출력\n",
        "model3.add(keras.layers.LSTM(8, dropout=0.3))\n",
        "model3.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model3.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpgpTheKMWqT"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "<h2> GRU (Gated Recurrent Unit) </h2>\n",
        "\n",
        "- LSTM의 간소화한 버전 (가중치, 파라미터 값이 LSTM보다 적어 계산량 적음)\n",
        "- 셀 상태를 계산하지 않고 은닉 상태 하나만 포함\n",
        "- 셀 3개: 2개의 시그모이드 활성화 함수 + 1개의 tanh 활성화 함수\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHvjhfMEMxAC",
        "outputId": "8c6f6368-718e-4bd8-b26c-b416dbc43d77"
      },
      "source": [
        "model4 = keras.Sequential()\n",
        "\n",
        "model4.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model4.add(keras.layers.GRU(8))\n",
        "model4.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model4.summary()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 100, 16)           8000      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 8)                 624       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 8,633\n",
            "Trainable params: 8,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWha3dy1MyOS",
        "outputId": "72cd2836-67a6-4f7b-fab5-db912384b134"
      },
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model4.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
        "               metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-gru-model.h5', \n",
        "                                                save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "history = model4.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
        "                     validation_data=(val_seq, val_target),\n",
        "                     callbacks=[checkpoint_cb, early_stopping_cb])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 14s 40ms/step - loss: 0.6924 - accuracy: 0.5236 - val_loss: 0.6914 - val_accuracy: 0.5532\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6904 - accuracy: 0.5632 - val_loss: 0.6893 - val_accuracy: 0.5672\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6874 - accuracy: 0.5840 - val_loss: 0.6858 - val_accuracy: 0.5898\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6826 - accuracy: 0.6000 - val_loss: 0.6807 - val_accuracy: 0.5984\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6754 - accuracy: 0.6152 - val_loss: 0.6729 - val_accuracy: 0.6154\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6647 - accuracy: 0.6296 - val_loss: 0.6614 - val_accuracy: 0.6256\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6492 - accuracy: 0.6431 - val_loss: 0.6450 - val_accuracy: 0.6362\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.6266 - accuracy: 0.6639 - val_loss: 0.6196 - val_accuracy: 0.6646\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.5918 - accuracy: 0.6882 - val_loss: 0.5794 - val_accuracy: 0.7042\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.5383 - accuracy: 0.7315 - val_loss: 0.5306 - val_accuracy: 0.7374\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.5069 - accuracy: 0.7563 - val_loss: 0.5127 - val_accuracy: 0.7534\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.4923 - accuracy: 0.7642 - val_loss: 0.5033 - val_accuracy: 0.7568\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4817 - accuracy: 0.7727 - val_loss: 0.4952 - val_accuracy: 0.7668\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4726 - accuracy: 0.7797 - val_loss: 0.4853 - val_accuracy: 0.7730\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.4656 - accuracy: 0.7828 - val_loss: 0.4800 - val_accuracy: 0.7782\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.4596 - accuracy: 0.7877 - val_loss: 0.4750 - val_accuracy: 0.7786\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4540 - accuracy: 0.7923 - val_loss: 0.4718 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.4493 - accuracy: 0.7948 - val_loss: 0.4671 - val_accuracy: 0.7826\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4451 - accuracy: 0.7985 - val_loss: 0.4644 - val_accuracy: 0.7796\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4416 - accuracy: 0.8011 - val_loss: 0.4633 - val_accuracy: 0.7796\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4390 - accuracy: 0.8016 - val_loss: 0.4634 - val_accuracy: 0.7806\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.4360 - accuracy: 0.8049 - val_loss: 0.4583 - val_accuracy: 0.7884\n",
            "Epoch 23/100\n",
            " 21/313 [=>............................] - ETA: 10s - loss: 0.4540 - accuracy: 0.7909"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNFXw126NX8r"
      },
      "source": [
        "test_seq = pad_sequences(test_input, maxlen=100)\n",
        "rnn_model = keras.models.load_model('best-2rnn-model.h5')\n",
        "rnn_model.evaluate(test_seq, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}